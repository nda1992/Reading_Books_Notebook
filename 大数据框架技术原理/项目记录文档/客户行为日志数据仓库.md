# 日志数据仓库

## 	数据仓库的基本理论

数据仓库，英文名称为Data Warehouse，可简写为DW或DWH.数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合.它出于分析性报告和决策支持目的而创建.为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制.<br>

### 数据仓库的特点

- **数据仓库的数据是面向主题的**
- **数据仓库的数据是集成的**
- **数据仓库的数据是不可更新的**
- **数据仓库的数据是随时间不断变化的**

### 数据仓库的分层

我们通过查阅一些资料和企业的工程业务情况.将数据仓库分为$4$层：

- 原始数据层(ODS)
- 明细数据层(DWD)
- 数据服务层(DWS)
- 数据应用层(ADS)

Note:数据仓库分层的作用？<br>

- 用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据
- 如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大
- 通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可

## 数据仓库的搭建

### 数据仓库中各层的表分布情况

![](/usr/2019/读书笔记/images/各层中表的分布情况.png)

对上述各个层的说明：

- ODS层：存储原始数据.数据不经过任何处理，直接将HDFS中的数据加载到Hive中.
- DWD层：结构和粒度与原表保持一致.针对ODS层中的数据进行清洗（去除脏数据、空值等等）.
- DWS层：以DWD层为基础.进行轻度汇总.将用户当日、设备当日、商家当日、商品当日的粒度.在这一层通过会以某个维度线索，从而组成跨主题的宽表.（一个用户的签到数、评论数、点赞数、浏览商品数组成的多列表）.
- ADS层：为各种统计报表提供数据（APP层）

### 其他补充

省略了Hive和MySQL的安装...

### ODS层

```sql
-- 创建数据库UserMall
CREATE DATABASE UserMall;
```

将原始数据分别加载到ods_start_log和ods_event_log两个表中.

```sql
DROP TABLE IF EXISTS ods_start_log;

-- 创建启动的外部表(ods_start_log)
CREATE EXTERNAL TABLE ods_start_log (`line` String)
PARTITIONED BY (`dt` String)
STORE AS 
INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/UserMall/ods/ods_start_log'

-- 加载数据
LOAD DATA INPATH '/origin_data/UserMall/log/topic_start/2019-02-10'
INTO TABLE UserMall.ods_start_log PARTITION(dt='2019-02-10');


-- 创建事件日志的外部表(ods_event_log)
DROP TABLE IF EXISTS ods_event_log;

CREATE EXTERNAL  TABLE ods_event_log (line String)
PARTITIONED BY (`dt` String)
STORE AS 
INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/UserMall/ods/ods_event_log';

-- 加载数据
LOAD DATA INPATH '/origin_data/UserMall/log/topic_event/2019-02-10'
INTO TABLE UserMall.ods_event_log PARTITION (dt='2019-02-10');
```

加载数据脚本

```shell
#!/bin/bash

APP = UserMall
hive=/root/opt/module/bin/hive
# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if[ -n "$1" ]
	do_date=$1
else
	do_date=`date -d "-1 day"+F%`
	echo "***************日志日期为 $do_date******************"
	sql="
	'LOAD DATA INPATH /origin_data/UserMall/log/topic_start/$do_date' INTO TABLE "$APP".ods_start_log PARTITION(dt='$do_date');
	'LOAD DATA INPATH /origin_data/UserMall/log/topic_event/$do_date' INTO TABLE "$APP".ods_event_log PARTITION(dt='$do_date');
	"
fi
$hive -e "$sql"
```

### DWD层

```sql
-- 创建dwd_start_log表
DROP TABLE IF EXISTS dwd_start_log;

CREATE EXTERNAL TABLE dwd_start_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`entry` string, 
`open_ad_type` string, 
`action` string, 
`loading_time` string, 
`detail` string, 
`extend1` string
)
PARTITIONED BY (dt='2019-02-10')
LOCATITION '/warehouse/UserMall/dwd/dwd_start_log';


-- 导入数据
INSERT OVERWIRTE TABLE dwd_start_log
PARTITION (dt='2019-02-10')
SELECT
	get_json_object(line,'$.mid') mid_id,
	get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t') app_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
FROM ods_start_log
WHERE dt='2019-02-10';
```

数据加载脚本

```shell
#!/bin/bash

# 定义变量方便修改
APP=gmall
hive=/opt/module/hive/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
	do_date=$1
else 
	do_date=`date -d "-1 day" +%F`  
fi 

sql="
set hive.exec.dynamic.partition.mode=nonstrict;

INSERT OVERWRITE TABLE "$APP".dwd_start_log
PARTITION (dt='$do_date')
SELECT 
    get_json_object(line,'$.mid') mid_id,
    get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t') app_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
FROM "$APP".ods_start_log 
WHERE dt='$do_date';
"

$hive -e "$sql"
```

### 解析DWD层中事件表

这里的采用了UDF和UTDF函数.<br>

首先：公共字段的解析使用UDF函数实现.<br>

其次：解析事件名称及其对应的json字段使用UDTF函数.<br>

```sql
-- 创建基础事件表
-- 其中event_name和event_json用来对应事件名和整个事件
DROP TABLE IF EXISTS dwd_base_event_log;

CREATE  EXTERNAL TABLE dwd_base_event_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string, 
`app_time` string, 
`network` string, 
`lng` string, 
`lat` string, 
`event_name` string, 
`event_json` string, 
`server_time` string)
PARTITIONED BY (dt='2019-02-10')
LOCATITION '/warehouse/UserMall/dwd/dwd_base_event_log';
```

接下来，我们使用UDF和UDTF函数分别解析出公共字段和事件名称、json字段.

UDF函数的代码

```java
package com.UserMall.udf;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.json.JSONException;
import org.json.JSONObject;

public class BaseFieldUDF extends UDF {

    public String evaluate(String line, String jsonkeysString) {
        
        // 0 准备一个sb
        StringBuilder sb = new StringBuilder();

        // 1 切割jsonkeys  mid uid vc vn l sr os ar md
        String[] jsonkeys = jsonkeysString.split(",");

        // 2 处理line   服务器时间 | json
        String[] logContents = line.split("\\|");

        // 3 合法性校验
        if (logContents.length != 2 || StringUtils.isBlank(logContents[1])) {
            return "";
        }

        // 4 开始处理json
        try {
            JSONObject jsonObject = new JSONObject(logContents[1]);

            // 获取cm里面的对象
            JSONObject base = jsonObject.getJSONObject("cm");

            // 循环遍历取值
            for (int i = 0; i < jsonkeys.length; i++) {
                String filedName = jsonkeys[i].trim();

                if (base.has(filedName)) {
                    sb.append(base.getString(filedName)).append("\t");
                } else {
                    sb.append("\t");
                }
            }

            sb.append(jsonObject.getString("et")).append("\t");
            sb.append(logContents[0]).append("\t");
        } catch (JSONException e) {
            e.printStackTrace();
        }

        return sb.toString();
    }

    public static void main(String[] args) {

        String line = "1541217850324|{\"cm\":{\"mid\":\"m7856\",\"uid\":\"u8739\",\"ln\":\"-74.8\",\"sv\":\"V2.2.2\",\"os\":\"8.1.3\",\"g\":\"P7XC9126@gmail.com\",\"nw\":\"3G\",\"l\":\"es\",\"vc\":\"6\",\"hw\":\"640*960\",\"ar\":\"MX\",\"t\":\"1541204134250\",\"la\":\"-31.7\",\"md\":\"huawei-17\",\"vn\":\"1.1.2\",\"sr\":\"O\",\"ba\":\"Huawei\"},\"ap\":\"weather\",\"et\":[{\"ett\":\"1541146624055\",\"en\":\"display\",\"kv\":{\"goodsid\":\"n4195\",\"copyright\":\"ESPN\",\"content_provider\":\"CNN\",\"extend2\":\"5\",\"action\":\"2\",\"extend1\":\"2\",\"place\":\"3\",\"showtype\":\"2\",\"category\":\"72\",\"newstype\":\"5\"}},{\"ett\":\"1541213331817\",\"en\":\"loading\",\"kv\":{\"extend2\":\"\",\"loading_time\":\"15\",\"action\":\"3\",\"extend1\":\"\",\"type1\":\"\",\"type\":\"3\",\"loading_way\":\"1\"}},{\"ett\":\"1541126195645\",\"en\":\"ad\",\"kv\":{\"entry\":\"3\",\"show_style\":\"0\",\"action\":\"2\",\"detail\":\"325\",\"source\":\"4\",\"behavior\":\"2\",\"content\":\"1\",\"newstype\":\"5\"}},{\"ett\":\"1541202678812\",\"en\":\"notification\",\"kv\":{\"ap_time\":\"1541184614380\",\"action\":\"3\",\"type\":\"4\",\"content\":\"\"}},{\"ett\":\"1541194686688\",\"en\":\"active_background\",\"kv\":{\"active_source\":\"3\"}}]}";
        String x = new BaseFieldUDF().evaluate(line, "mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,nw,ln,la,t");
        System.out.println(x);
    }
}
```

UDTF的实现

```java
package com.UserMall.udtf;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
import org.json.JSONArray;
import org.json.JSONException;

import java.util.ArrayList;

public class EventJsonUDTF extends GenericUDTF {

    //该方法中，我们将指定输出参数的名称和参数类型：
    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {

        ArrayList<String> fieldNames = new ArrayList<String>();
        ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();

        fieldNames.add("event_name");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        fieldNames.add("event_json");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    //输入1条记录，输出若干条结果
    @Override
    public void process(Object[] objects) throws HiveException {

        // 获取传入的et
        String input = objects[0].toString();

        // 如果传进来的数据为空，直接返回过滤掉该数据
        if (StringUtils.isBlank(input)) {
            return;
        } else {

            try {
                // 获取一共有几个事件（ad/facoriters）
                JSONArray ja = new JSONArray(input);

                if (ja == null)
                    return;

                // 循环遍历每一个事件
                for (int i = 0; i < ja.length(); i++) {
                    String[] result = new String[2];

                    try {
                        // 取出每个的事件名称（ad/facoriters）
                        result[0] = ja.getJSONObject(i).getString("en");

                        // 取出每一个事件整体
                        result[1] = ja.getString(i);
                    } catch (JSONException e) {
                        continue;
                    }

                    // 将结果返回
                    forward(result);
                }
            } catch (JSONException e) {
                e.printStackTrace();
            }
        }
    }

    //当没有记录处理的时候该方法会被调用，用来清理代码或者产生额外的输出
    @Override
    public void close() throws HiveException {

    }
}
```



```sql
-- 在Hive中建立临时函数
create temporary function base_analizer as 'com.UserMall.udf.BaseFieldUDF';

create temporary function flat_analizer as 'com.UserMall.udtf.EventJsonUDTF';

-- 向dwd_base_event_log表插入数据
INSERT INTO OVERWRITE TABLE dwd_base_event_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
event_name,
event_json,
server_time
FROM (
    SELELCT
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[0]   as mid_id,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[1]   as user_id,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[2]   as version_code,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[3]   as version_name,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[4]   as lang,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[5]   as source,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[6]   as os,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[7]   as area,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[8]   as model,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[9]   as brand,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[10]   as sdk_version,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[11]  as gmail,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[12]  as height_width,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[13]  as app_time,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[14]  as network,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[15]  as lng,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[16]  as lat,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[17]  as ops,
split(base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la'),'\t')[18]  as server_time
FROM dwd_event_log
WHERE dt='2019-02-10' AND base_analizer(line,'mid,uid,vc,vn,l,sr,os,ar,md,ba,sv,g,hw,t,nw,ln,la')<>'
)
sdk_log lateral view flat_analizer(ops) tmp_k as event_name, event_json;
```

### 将DWD层中的dwd_base_event_log表拆分成不同的事件表

我们根据dwd_base_event_log表中的不同字段拆分成不同的表.名称为dwd_xxx_log.<br>

#### 商品点击表

```sql
DROP TABLE if EXISTS dwd_display_log;

CREATE EXTERNAL TABLE dwd_display_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
`app_time` string,
`network` string,
`lng` string,
`lat` string,
`action` string,
`goodsid` string,
`place` string,
`extend1` string,
`category` string,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_display_log/';

-- 导入数据
INSERT OVERWRITE TABLE dwd_display_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.goodsid') goodsid,
get_json_object(event_json,'$.kv.place') place,
get_json_object(event_json,'$.kv.extend1') extend1,
get_json_object(event_json,'$.kv.category') category,
server_time
FROM dwd_base_event_log 
WHERE dt='2019-02-10' AND event_name='display';
```

#### 商品详情表

```sql
DROP TABLE IF EXISTS dwd_newsdetail_log;

CREATE EXTERNAL TABLE dwd_newsdetail_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string, 
`app_time` string,  
`network` string, 
`lng` string, 
`lat` string, 
`entry` string,
`action` string,
`goodsid` string,
`showtype` string,
`news_staytime` string,
`loading_time` string,
`type1` string,
`category` string,
`server_time` string)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_newsdetail_log/';

-- 导入数据
INSERT OVERWRITE TABLE dwd_newsdetail_log
PARTITION (dt='2019-02-10')
select 
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.entry') entry,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.goodsid') goodsid,
get_json_object(event_json,'$.kv.showtype') showtype,
get_json_object(event_json,'$.kv.news_staytime') news_staytime,
get_json_object(event_json,'$.kv.loading_time') loading_time,
get_json_object(event_json,'$.kv.type1') type1,
get_json_object(event_json,'$.kv.category') category,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10' AND event_name='newsdetail';
```

#### 商品列表页表

```SQL
DROP TABLE IF EXISTS dwd_loading_log;

CREATE EXTERNAL TABLE dwd_loading_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string,
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`action` string,
`loading_time` string,
`loading_way` string,
`extend1` string,
`extend2` string,
`type` string,
`type1` string,
`server_time` string)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_loading_log/';

--导入数据
INSERT OVERWRITE TABLE dwd_loading_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.loading_time') loading_time,
get_json_object(event_json,'$.kv.loading_way') loading_way,
get_json_object(event_json,'$.kv.extend1') extend1,
get_json_object(event_json,'$.kv.extend2') extend2,
get_json_object(event_json,'$.kv.type') type,
get_json_object(event_json,'$.kv.type1') type1,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10'AND event_name='loading';
```

#### 广告表

```SQL
DROP TABLE IF EXISTS dwd_ad_log;


CREATE EXTERNAL TABLE dwd_ad_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`entry` string,
`action` string,
`content` string,
`detail` string,
`ad_source` string,
`behavior` string,
`newstype` string,
`show_style` string,
`server_time` string)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_ad_log/';

--导入数据

INSERT OVERWRITE TABLE dwd_ad_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.entry') entry,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.content') content,
get_json_object(event_json,'$.kv.detail') detail,
get_json_object(event_json,'$.kv.source') ad_source,
get_json_object(event_json,'$.kv.behavior') behavior,
get_json_object(event_json,'$.kv.newstype') newstype,
get_json_object(event_json,'$.kv.show_style') show_style,
server_time
FROM dwd_base_event_log 
WHERE dt='2019-02-10'AND event_name='ad';
```

#### 消息通知表

```SQL
DROP TABLE IF EXISTS dwd_notification_log;
CREATE EXTERNAL TABLE dwd_notification_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string,
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`action` string,
`noti_type` string,
`ap_time` string,
`content` string,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_notification_log/';

-- 导入数据
INSERT OVERWRITE TABLE dwd_notification_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.noti_type') noti_type,
get_json_object(event_json,'$.kv.ap_time') ap_time,
get_json_object(event_json,'$.kv.content') content,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10' AND event_name='notification';
```

#### 用户前台活跃表

```SQL
DROP TABLE IF EXISTS dwd_active_foreground_log;
CREATE EXTERNAL TABLE dwd_active_foreground_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
`app_time` string,
`network` string,
`lng` string,
`lat` string,
`push_id` string,
`access` string,
`server_time` string)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_foreground_log/';

--导入数据
INSERT OVERWRITE TABLE dwd_active_foreground_log
PARTITION (dt='2019-02-10')
select 
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.push_id') push_id,
get_json_object(event_json,'$.kv.access') access,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10' AND event_name='active_foreground';
```

#### 用户后台活跃表

```SQL
DROP TABLE IF EXISTS dwd_active_background_log;
CREATE EXTERNAL TABLE dwd_active_background_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
 `height_width` string,
`app_time` string,
`network` string,
`lng` string,
`lat` string,
`active_source` string,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_background_log/';

-- 导入数据
INSERT OVERWRITE TABLE dwd_active_background_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.active_source') active_source,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10' AND event_name='active_background';
```

#### 评论表

```sql
DROP TABLE IF EXISTS dwd_comment_log;
CREATE EXTERNAL TABLE dwd_comment_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
`app_time` string,
`network` string,
`lng` string,
`lat` string,
`comment_id` int,
`userid` int,
`p_comment_id` int, 
`content` string,
`addtime` string,
`other_id` int,
`praise_count` int,
`reply_count` int,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_comment_log/';

--导入数据
INSERT OVERWRITE TABLE dwd_comment_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.comment_id') comment_id,
get_json_object(event_json,'$.kv.userid') userid,
get_json_object(event_json,'$.kv.p_comment_id') p_comment_id,
get_json_object(event_json,'$.kv.content') content,
get_json_object(event_json,'$.kv.addtime') addtime,
get_json_object(event_json,'$.kv.other_id') other_id,
get_json_object(event_json,'$.kv.praise_count') praise_count,
get_json_object(event_json,'$.kv.reply_count') reply_count,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10'AND event_name='comment';
```

#### 收藏表

```SQL
DROP TABLE IF EXISTS dwd_favorites_log;
CREATE EXTERNAL TABLE dwd_favorites_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`id` int, 
`course_id` int, 
`userid` int,
`add_time` string,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_favorites_log/';

-- 导入数据
INSERT OVERWRITE TABLE dwd_favorites_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.id') id,
get_json_object(event_json,'$.kv.course_id') course_id,
get_json_object(event_json,'$.kv.userid') userid,
get_json_object(event_json,'$.kv.add_time') add_time,
server_time
FROM dwd_base_event_log 
WHERE dt='2019-02-10' AND event_name='favorites';
```

#### 点赞表

```SQL
DROP TABLE IF EXISTS dwd_praise_log;
CREATE EXTERNAL TABLE dwd_praise_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`id` string, 
`userid` string, 
`target_id` string,
`type` string,
`add_time` string,
`server_time` string
)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_praise_log/';

--导入数据
INSERT OVERWRITE TABLE dwd_praise_log
PARTITION (dt='2019-02-10')
SELECT 
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.id') id,
get_json_object(event_json,'$.kv.userid') userid,
get_json_object(event_json,'$.kv.target_id') target_id,
get_json_object(event_json,'$.kv.type') type,
get_json_object(event_json,'$.kv.add_time') add_time,
server_time
FROM dwd_base_event_log
WHERE dt='2019-02-10' AND event_name='praise';
```

#### 错误日志表

```SQL
DROP TABLE IF EXISTS dwd_error_log;
CREATE EXTERNAL TABLE dwd_error_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`errorBrief` string, 
`errorDetail` string, 
`server_time` string)
PARTITIONED BY (dt string)
LOCATION '/warehouse/UserMall/dwd/dwd_error_log/';


--导入数据
INSERT OVERWRITE TABLE dwd_error_log
PARTITION (dt='2019-02-10')
SELECT
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.errorBrief') errorBrief,
get_json_object(event_json,'$.kv.errorDetail') errorDetail,
server_time
FROM dwd_base_event_log 
WHERE dt='2019-02-10' AND event_name='error';
```

## 相关指标统计分析

为了使得思路更加清晰，我们先作出了如下各个表之间的关系结构图.

![](/usr/2019/读书笔记/images/表之间的关系图.png)

项目中使用Hive作为统计分析工具，对用户行为的相关指标进行了分析.<br>

### 用户的活跃度

基于DWD层中的dwd_start_log表分别统计了用户的每日、每周和每月的活跃度.

#### 用户的每日活跃分析

原理：只要dwd_start_log表中的mid_id字段不为空，则表示该用户的活跃的，统计出所有mid_id不为空的用户即可.

```sql
DROP TABLE IF EXISTS dws_uv_detail_day;

CREATE EXTERNAL TABLE dws_uv_detail_day
(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    `app_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度'
)
PARTITION BY (dt string)
STORES AS PARQUE
LOCATION '/warehouse/UserMall/dws/dws_uv_detail_day';

-- 导入数据
INSERT OVERWRITE TABLE dws_uv_detail_day
PARTITION (dt='2019-02-10')
SELECT
	mid_id,
	concat_ws('|', collect_set(user_id)) user_id,
    concat_ws('|', collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set(app_time)) app_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat
FROM dwd_start_log
WHERE dt='2019-02-10'
GROUP BY mid_id;
```

#### 用户的每周活跃分析

根据用户访问明细，从而获取用户每周的活跃度.

```sql
DROP TABLE IF EXISTS dws_uv_detail_wk;

INSERT EXTERNAL TABLE dws_uv_detail_wk( 
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    `app_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度',
    `monday_date` string COMMENT '周一日期',
    `sunday_date` string COMMENT  '周日日期' 
) COMMENT '用户每周的活跃度'
PARTITIONED BY (`wk_dt` string)
STORED AS parquet
LOCATION '/warehouse/UserMall/dws/dws_uv_detail_wk/';

--导入数据

INSERT OVERWRITE TABLE dws_uv_detail_wk partition(wk_dt)
SELECT  
    mid_id,
    concat_ws('|', collect_set(user_id)) user_id,
    concat_ws('|', collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang)) lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set(app_time)) app_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_add(next_day('2019-02-10','MO'),-7),
    date_add(next_day('2019-02-10','MO'),-1),
    concat(date_add( next_day('2019-02-10','MO'),-7), '_' , date_add(next_day('2019-02-10','MO'),-1) 
)
FROM dws_uv_detail_day 
WHERE dt>=date_add(next_day('2019-02-10','MO'),-7) AND dt<=date_add(next_day('2019-02-10','MO'),-1) 
GROUP BY mid_id;
```

#### 用户的每月活跃分析

使用date_format函数获取月份，并使用where条件获取该月内的活跃用户.

```sql
DROP TABLE IF EXISTS dws_uv_detail_mn;

CREATE EXTERNAL TABLE dws_uv_detail_mn( 
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    `app_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度'
) COMMENT '活跃用户按月明细'
PARTITIONED BY (`mn` string)
STORED AS parquet
location '/warehouse/UserMall/dws/dws_uv_detail_mn/';

-- 导入数据
INSERT OVERWRITE TABLE dws_uv_detail_mn partition(mn)
SELECT 
    mid_id,
    concat_ws('|', collect_set(user_id)) user_id,
    concat_ws('|', collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang)) lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set(app_time)) app_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_format('2019-02-10','yyyy-MM')
FROM dws_uv_detail_day
WHERE date_format(dt,'yyyy-MM') = date_format('2019-02-10','yyyy-MM')
GROUP BY mid_id;
```

#### 每日用户的新增人数

计算的方法是使用dws_uv_detail_day表左连接当前日期的dws_new_mid_day表（新表），如果mid_id为空，则表示这些用户的当日新增用户.

```sql
DROP TABLE IF EXISTS dws_new_mid_day;
CREATE EXTERNAL TABLE dws_new_mid_day
(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    `app_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度',
    `create_date`  string  comment '创建时间' 
)  COMMENT '每日新增设备信息'
STORED AS parquet
LOCATION '/warehouse/UserMall/dws/dws_new_mid_day/';

--导入数据
INSERT INTO TABLE dws_new_mid_day
select  
    ud.mid_id,
    ud.user_id , 
    ud.version_code , 
    ud.version_name , 
    ud.lang , 
    ud.source, 
    ud.os, 
    ud.area, 
    ud.model, 
    ud.brand, 
    ud.sdk_version, 
    ud.gmail, 
    ud.height_width,
    ud.app_time,
    ud.network,
    ud.lng,
    ud.lat,
    '2019-02-10'
FROM dws_uv_detail_day ud left JOIN dws_new_mid_day nm ON ud.mid_id=nm.mid_id
WHERE ud.dt='2019-02-10' AND nm.mid_id IS NULL;
```

### ADS层分析

在ADS层上将用户的日、周、月的活跃度汇总到一张表中（ads_uv_count表）.将DWS层上的三张表进行join连接.同时，使用COUNT(*)的方法求出所有的行数就是用户的日、周、月的活跃度总和.JOIN的连接条件是dt.

```sql
DROP TABLE IF EXISTS ads_uv_count;
CREATE EXTERNAL TABLE ads_uv_count( 
    `dt` string COMMENT '统计日期',
    `day_count` bigint COMMENT '当日用户数量',
    `wk_count`  bigint COMMENT '当周用户数量',
    `mn_count`  bigint COMMENT '当月用户数量',
    `is_weekend` string COMMENT 'Y,N是否是周末,用于得到本周最终结果',
    `is_monthend` string COMMENT 'Y,N是否是月末,用于得到本月最终结果' 
) COMMENT '活跃设备数'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/UserMall/ads/ads_uv_count/';


--导入数据
INSERT OVERWRITE TABLE ads_uv_count 
SELECT 
  '2019-02-10' dt,
   daycount.ct,
   wkcount.ct,
   mncount.ct,
   if(date_add(next_day('2019-02-10','MO'),-1)='2019-02-10','Y','N') ,
   if(last_day('2019-02-10')='2019-02-10','Y','N') 
FROM (
   SELECT  
      '2019-02-10' dt,
       COUNT(*) ct
   FROM dws_uv_detail_day
   WHERE dt='2019-02-10' )daycount JOIN ( 
   SELECT  
     '2019-02-10' dt,
     COUNT(*) ct
   FROM dws_uv_detail_wk
   WHERE wk_dt=concat(date_add(next_day('2019-02-10','MO'),-7),'_' ,date_add(next_day('2019-02-10','MO'),-1) )
) wkcount ON daycount.dt=wkcount.dt
JOIN 
( 
  SELECT  
     '2019-02-10' dt,
     COUNT (*) ct
   FROM dws_uv_detail_mn
   WHERE mn=date_format('2019-02-10','yyyy-MM')  
)mncount ON daycount.dt=mncount.dt;
```

#### ADS层的新增用户统计

将DWD层中的dws_new_mid_day表汇总到ADS层中的ads_new_mid_count表.

```sql
DROP TABLE IF EXISTS ads_new_mid_count;
CREATE EXTERNAL TABLE ads_new_mid_count
(
    `create_date`     string comment '创建时间' ,
    `new_mid_count`   BIGINT comment '新增设备数量' 
)  COMMENT '每日新增设备信息数量'
ROW FORMAR DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/UserMall/ads/ads_new_mid_count/';

--导入数据
INSERT INTO TABLE ads_new_mid_count
SELECT
	create_date,
	COUNT(*)
FROM dws_date='2019-02-10'
GROUP BY create_date;
```

### 用户的留存率分析

留存用户：某段时间内的新增用户，经过一段时间后，又继续使用应用的被认为是留存用户.<br>

留存率：留存用户占当时新增用户的比例.<br>

对当天的用户留存率的计算方法（假设根据$10$日统计$11​$日的留存率）：<br>

10日的新增设备留存率=10日的新增设备而且在11日活跃的/10日的新增设备<br>

10日的新增设备：10日活跃表left join每日的新增表（新增表的mid_id为空的就是10的新增用户）.<br>

10日的新增设备而且在11日活跃的：10日的新增 join 11日的活跃而且新增日期是10日，活跃日期是11日.

```sql
DROP TABLE IF EXISTS dws_user_retention_day;
CREATE EXTERNAL TABLE dws_user_retention_day 
(
`mid_id` string COMMENT '设备唯一标识',
`user_id` string COMMENT '用户标识', 
 `version_code` string COMMENT '程序版本号', 
 `version_name` string COMMENT '程序版本名', 
`lang` string COMMENT '系统语言', 
`source` string COMMENT '渠道号', 
`os` string COMMENT '安卓系统版本', 
`area` string COMMENT '区域', 
`model` string COMMENT '手机型号', 
`brand` string COMMENT '手机品牌', 
`sdk_version` string COMMENT 'sdkVersion', 
`gmail` string COMMENT 'gmail', 
`height_width` string COMMENT '屏幕宽高',
`app_time` string COMMENT '客户端日志产生时的时间',
`network` string COMMENT '网络模式',
`lng` string COMMENT '经度',
`lat` string COMMENT '纬度',
`create_date`    string  comment '设备新增时间',
`retention_day`  INT comment '截止当前日期留存天数'
)  COMMENT '每日用户留存情况'
PARTITIONED BY (`dt` string)
STORED AS parquet
LOCATION '/warehouse/UserMall/dws/dws_user_retention_day/';

--导入数据（每天计算前1天的新用户访问留存明细）

INSERT OVERWRITE TABLE dws_user_retention_day
PARTITION (dt="2019-02-11")
SELECT 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm.app_time,
    nm.network,
    nm.lng,
    nm.lat,
	nm.create_date,
	1 retention_day 
FROM dws_uv_detail_day ud JOIN dws_new_mid_day nm  ON ud.mid_id =nm.mid_id 
WHERE ud.dt='2019-02-11' AND nm.create_date=date_add('2019-02-11',-1);	--表示10号注册，在11号注册的用户

-- 导入数据（每天计算前1,2,3，n天的新用户访问留存明细），根据1\2\3...天的情况将所有表union到一张表中
INSERT OVERWRITE TABLE dws_user_retention_day
PARTITION(dt="2019-02-11")
SELECT
    nm.mid_id,
    nm.user_id,
    nm.version_code,
    nm.version_name,
    nm.lang,
    nm.source,
    nm.os,
    nm.area,
    nm.model,
    nm.brand,
    nm.sdk_version,
    nm.gmail,
    nm.height_width,
    nm.app_time,
    nm.network,
    nm.lng,
    nm.lat,
    nm.create_date,
    1 retention_day 
FROM dws_uv_detail_day ud JOIN dws_new_mid_day nm  ON ud.mid_id =nm.mid_id 
WHERE ud.dt='2019-02-11' AND nm.create_date=date_add('2019-02-11',-1)

UNION ALL
SELECT 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm.app_time,
    nm.network,
    nm.lng,
    nm.lat,
    nm.create_date,
    2 retention_day 
FROM  dws_uv_detail_day ud JOIN dws_new_mid_day nm   ON ud.mid_id =nm.mid_id 
WHERE ud.dt='2019-02-11' AND nm.create_date=date_add('2019-02-11',-2)

UNION ALL
SELECT 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm.app_time,
    nm.network,
    nm.lng,
    nm.lat,
    nm.create_date,
    3 retention_day 
FROM  dws_uv_detail_day ud JOIN dws_new_mid_day nm  ON ud.mid_id =nm.mid_id 
WHERE ud.dt='2019-02-11' AND nm.create_date=date_add('2019-02-11',-3);
```

### ADS层统计用户留存率

- 先统计出用户留存的总数

```sql
DROP TABLE IF EXISTS ads_user_retention_day_count;
CREATE EXTERNAL TABLE ads_user_retention_day_count 
(
   `create_date`       string  comment '设备新增日期',
   `retention_day`     int comment '截止当前日期留存天数',
   `retention_count`    bigint comment  '留存数量'
)  COMMENT '每日用户留存情况'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/UserMall/ads/ads_user_retention_day_count/';


-- 导入数据
INSERT INTO TABLE ads_user_retention_day_count
SELECT
	create_date,
	retention_day,
	COUNT(*) retention_count
FROM dws_user_retention_day
WHERE dt='2019-02-11'
GROUP BY create_date,retention_day;
```

- 用户留存比率

```sql
DROP TABLE IF EXISTS ads_user_retention_day_rate;
CREATE EXTERNAL TABLE ads_user_retention_day_rate 
(
     `stat_date`          string comment '统计日期',
     `create_date`       string  comment '设备新增日期',
     `retention_day`     int comment '截止当前日期留存天数',
     `retention_count`    bigint comment  '留存数量',
     `new_mid_count`     bigint comment '当日设备新增数量',
     `retention_ratio`   decimal(10,2) comment '留存率'
)  COMMENT '每日用户留存情况'
ROW FORMAR DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/warehouse/UserMall/ads/ads_user_retention_day_rate/';

--导入数据
INSERT INTO TABLE  ads_user_retention_day_rate
SELECT 
    '2019-02-11', 
    ur.create_date,
    ur.retention_day, 
    ur.retention_count, 
    nc.new_mid_count,
    ur.retention_count/nc.new_mid_count*100
FROM 
(
    SELECT
        create_date,
        retention_day,
        COUNT(*) retention_count
   FROM dws_user_retention_day
   WHERE dt='2019-02-11' 
    GROUP BY create_date,retention_day
) ur JOIN ads_new_mid_count nc ON nc.create_date=ur.create_date;
```

## 总结

我们主要总结了关于Hive的一些优化问题，Hive的优化来源于网上的资料.<br>

#### MapJoin

如果不指定MapJoin或者不符合MapJoin的条件,那么Hive解析器会将Join操作转换成Common Join,即:在Reduce阶段完成join.容易发生数据倾斜.可以用MapJoin把小表全部加载到内存在map端进行join,避免reducer处理.

#### 行列过滤

列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *<br>

行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤<br>

#### 分桶技术

#### 分区技术

#### 合理设置Map的数量

- 通常情况下，作业会通过input的目录产生一个或者多个map任务

主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小

- map的数量要设置合理

如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块,用一个map任务来完成,而一个map任务启动和初始化的时间远远大于逻辑处理的时间,就会造成很大的资源浪费.而且,同时可执行的map数是受限的.

- 是不是保证每个map处理接近128m的文件块，就高枕无忧了？

不是.比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时.

- 小文件进行合并

在Map执行前合并小文件,减少Map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）.HiveInputFormat没有对小文件合并功能.

#### 合理设置Reduce的数量

Reduce个数并不是越多越好<br>

（1）过多的启动和初始化Reduce也会消耗时间和资源；<br>

（2）另外，有多少个Reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；<br>

在设置Reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的Reduce数；使单个Reduce任务处理数据量大小要合适.

#### 常用参数

```xml
SET hive.merge.mapfiles = true; -- 默认true，在map-only任务结束时合并小文件
SET hive.merge.mapredfiles = true; -- 默认false，在map-reduce任务结束时合并小文件
SET hive.merge.size.per.task = 268435456; -- 默认256M
SET hive.merge.smallfiles.avgsize = 16777216; -- 当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge
```