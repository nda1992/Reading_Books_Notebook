# 数据采集平台

[TOC]

## 数据仓库的整体流程

![](/usr/2019/读书笔记/images/数据仓库整体流程.png)

数据仓库是为企业所有决策过程提供所有系统数据支持的战略集合.数据仓库数据数据的最终目的地，而是为数据最终的目的地做好准备，这些准备包括：清洗、转义、重组、合并和统计等.

## 项目需求

#### 实现数据采集平台的搭建

#### 实现用户行为数据仓库的分层搭建

#### 依据数据仓库的数据进行用户活跃度、留存率等报表分析

## 涉及的技术框架

- 数据采集：Flume、Kafka
- 数据存储：MySQL、HDFS
- 数据统计：Hive

## 项目整体框架

![](/usr/2019/读书笔记/images/数据仓库项目整体流程.png)

## 技术框架的版本

| 技术框架  | 版本     |
| --------- | -------- |
| Hadoop    | 2.7.2    |
| Flume     | 1.7.0    |
| Kafka     | 0.11.0.2 |
| Hive      | 1.2.1    |
| MySQL     | 5.6.24   |
| Java      | 1.8      |
| zookeeper | 3.4.10   |

## 各个技术框架在服务器上的进程分配情况

| 服务名称           | 子服务                | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ------------------ | --------------------- | --------------- | --------------- | --------------- |
| HDFS               | NameNode              | √               |                 |                 |
|                    | DataNode              | √               | √               | √               |
|                    | SecondaryNameNode     |                 |                 | √               |
| Yarn               | NodeManager           | √               | √               | √               |
|                    | Resourcemanager       |                 | √               |                 |
| Zookeeper          | Zookeeper Server      | √               | √               | √               |
| Flume(采集日志)    | Flume                 | √               | √               |                 |
| Kafka              | Kafka                 | √               | √               | √               |
| Flume（消费Kafka） | Flume                 |                 |                 | √               |
| Hive               | Hive                  | √               |                 |                 |
| MySQL              | MySQL                 | √               |                 |                 |
| Sqoop              | Sqoop                 | √               |                 |                 |
| Presto             | Coordinator           | √               |                 |                 |
|                    | Worker                |                 | √               | √               |
| Azkaban            | AzkabanWebServer      | √               |                 |                 |
|                    | AzkabanExecutorServer | √               |                 |                 |
| Druid              | Druid                 | √               | √               | √               |
| 服务数总计         |                       | 13              | 8               | 9               |

## 数据的基本格式

包括公共字段和事件字段

```json
{
"ap":"xxxxx",//项目数据来源 app pc
"cm": {  //公共字段
		"mid": "",  // (String) 设备唯一标识
        "uid": "",  // (String) 用户标识
        "vc": "1",  // (String) versionCode，程序版本号
        "vn": "1.0",  // (String) versionName，程序版本名
        "l": "zh",  // (String) 系统语言
        "sr": "",  // (String) 渠道号，应用从哪个渠道来的。
        "os": "7.1.1",  // (String) Android系统版本
        "ar": "CN",  // (String) 区域
        "md": "BBB100-1",  // (String) 手机型号
        "ba": "blackberry",  // (String) 手机品牌
        "sv": "V2.2.1",  // (String) sdkVersion
        "g": "",  // (String) gmail
        "hw": "1620x1080",  // (String) heightXwidth，屏幕宽高
        "t": "1506047606608",  // (String) 客户端日志产生时的时间
        "nw": "WIFI",  // (String) 网络模式
        "ln": 0,  // (double) lng经度
        "la": 0  // (double) lat 纬度
    },
"et":  [  //事件
            {
                "ett": "1506047605364",  //客户端事件产生时间
                "en": "display",  //事件名称
                "kv": {  //事件结果，以key-value形式自行定义
                    "goodsid": "236",
                    "action": "1",
                    "extend1": "1",
"place": "2",
"category": "75"
                }
            }
        ]
}
```

对于事件字段，共分为11种类型.

- 商品列表
- 商品点击
- 商品详情
- 广告
- 消息通知
- 用户前台活跃
- 用户后台活跃
- 评论
- 收藏
- 点赞
- 错误日志

对于启动日志，包含了公共字段部分.

## 模拟数据

### Maven中的pom.xml

```xml
<!--版本号统一-->
<properties>
    <slf4j.version>1.7.20</slf4j.version>
    <logback.version>1.0.7</logback.version>
</properties>

<dependencies>
    <!--阿里巴巴开源json解析框架-->
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.51</version>
    </dependency>

    <!--日志生成框架-->
    <dependency>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-core</artifactId>
        <version>${logback.version}</version>
    </dependency>
    <dependency>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-classic</artifactId>
        <version>${logback.version}</version>
    </dependency>
</dependencies>

<!--编译打包插件-->
<build>
    <plugins>
        <plugin>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin </artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
                <archive>
                    <manifest>
                        <mainClass>com.UserMall.AppMain</mainClass>
                    </manifest>
                </archive>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

`数据生成的代码过长，此处没有列出.`<br>

## 数据采集过程

技术框架的安装部分也会省略.只写出了一些比较重要的内容.

### HDFS的写性能测试

```bash
/root/opt/module/hadoop/hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB
```

### HDFS的读性能测试

```bash
/root/opt/module/hadoop/hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
```

### 关于Hadoop中的一些参数调优方案

#### HDFS参数调优hdfs-site.xml

- dfs.namenode.handler.count=20 * log2(Cluster Size)，比如集群规模为8台时，此参数设置为60
- 编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟

#### YARN参数调优yarn-site.xml

（1）情景描述：总共7台机器,每天几亿条数据,数据源->Flume->Kafka->HDFS->Hive<br>

面临问题：数据统计主要用HiveSQL,没有数据倾斜,小文件已经做了合并处理,开启的JVM重用,而且IO没有阻塞,内存用了不到50%.但是还是跑的非常慢，而且数据量洪峰过来时,整个集群都会宕掉。基于这种情况有没有优化方案<br>

（2）解决办法：<br>

内存利用率不够.这个一般是Yarn的2个配置造成的,单个任务可以申请的最大内存大小,和Hadoop单个节点可用内存大小.调节这两个参数能提高系统内存的利用率<br>

（a）yarn.nodemanager.resource.memory-mb<br>

表示该节点上YARN可使用的物理内存总量,默认是8192（MB）,注意,如果你的节点内存资源不够8GB，则需要调减小这个值,而YARN不会智能的探测节点的物理内存总量<br>

（b）yarn.scheduler.maximum-allocation-mb<br>

单个任务可申请的最多物理内存量,默认是8192（MB）<br>

#### Hadoop宕机

（1）如果MR造成系统宕机.此时要控制Yarn同时运行的任务数,和每个任务申请的最大内存.调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量,默认是8192MB）<br>

（2）如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度.高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上.<br>

### Zookeeper

安装过程省略...<br>

#### Zookeeper集群的启动脚本

```shell
#!/bin/bash

case $1 in
"start"){
    for i in hadoop102 hadoop103 hadoop104
    do
    	ssh $i "/root/opt/module/zookeeper/bin/zkServer.sh start"
    done
};;
"stop"){
    for i in hadoop102 hadoop103 hadoop104
    do
    	ssh $i "/root/opt/module/zookeeper/bin/zkServer.sh stop"
    done
};;
"status"){
    for i in hadoop102 hadoop103 hadoop104
    do
    	ssh $i "/root/opt/module/zookeeper/bin/zkServer.sh status"
    done
};;
esac
```

### 日志生成

将编写好日志生成的代码文件打成jar包，上传到服务器上.并执行如下命令生成日志文件：

```bash
java -classpath log-produce.jar com.UserMall.AppMain >/root/opt/module/web.log
```

集群日志生成脚本

```shell
#!/bin/bash
for i in hadoop102 hadoop103 hadoop104
	do
		ssh $i "java -classpath /root/opt/module/log-produce.jar com.UserMall.AppMain $1 $2 >/root/opt/module/web.log"
	done
```

### 使用Flume采集日志

对于Flume中的Source组件，使用了Taildir Source，因为Taildir Source可以实现断点续传、多目录等.而Channel使用了Kafka Channel，这样省去了Sink部分，提高了效率.

#### Flume采集日志的基本流程

![](/usr/2019/读书笔记/images/FLume日志采集流程.png)



其中，log日志的格式为web-yyyy-mm-dd.log

#### Flume日志采集的配置(file-flume-kafka.conf)

```conf
a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /root/opt/module/flume/test/log_position.json
a1.sources.r1.filegroups = f1
#读取日志的位置
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor(拦截器配置)
a1.sources.r1.interceptors =  i1 i2
#ETL拦截器
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.LogETLInterceptor$Builder
#日志类型拦截器
a1.sources.r1.interceptors.i2.type = com.atguigu.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
#根据不同的类型发往不同的Channel
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer
```

`ETL和分类拦截器的代码此处没有给出`

#### Flume日志采集启动脚本

```shell
#!/bin/bash

case $1 in
"start"){
    for i in hadoop102 hadoop103
    do
    	echo " --------启动 $i 采集flume-------"
        ssh $i "nohup /root/opt/module/flume/bin/flume-ng agent --conf-file /root/opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/dev/null 2>&1 &"
    done
};;
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs kill"
        done

};;
esac
```

### Kafka

Kafka的安装省略...<br>

#### Kafka的启动脚本

```shell
#!/bin/bash

case $1 in
"start"){
    for i in hadoop102 hadoop103 hadoop104
    do
    	echo " --------启动 $i Kafka-------"
                # 用于KafkaManager监控
        ssh $i "export JMX_PORT=9988 && /root/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties "
    done
};;
"stop"){
    for i in hadoop102 hadoop103 hadoop104
        do
                echo " --------停止 $i Kafka-------"
                ssh $i "/root/opt/module/kafka/bin/kafka-server-stop.sh stop"
        done
};;
esac
```

#### Kafka的启动停止脚本

```shell
#! /bin/bash
case $1 in
"start"){
        echo " -------- 启动 KafkaManager -------"
        nohup /root/opt/module/kafka-manager-1.3.3.22/bin/kafka-manager   -Dhttp.port=7456 >start.log 2>&1 &
};;
"stop"){
        echo " -------- 停止 KafkaManager -------"
        ps -ef | grep ProdServerStart | grep -v grep |awk '{print $2}' | xargs kill 
};;
```

### 使用Flume消费Kafka中的数据

此次使用hadoop104作为日志消费者.<br>

![](/usr/2019/读书笔记/images/Flume消费kafka.png)

配置Flume消费kafka中的数据（kafka-flume-hdfs.conf）,最终存储到HDFS中.

```
## 组件
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

## source1,kafka的start主题
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_start

## source2，kafka的event主题
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics=topic_event

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir =/root /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /root/opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6

## channel2
a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /root/opt/module/flume/checkpoint/behavior2
a1.channels.c2.dataDirs =/root /opt/module/flume/data/behavior2/
a1.channels.c2.maxFileSize = 2146435071
a1.channels.c2.capacity = 1000000
a1.channels.c2.keep-alive = 6

## sink1，输出到HDFS中的配置
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/UserMall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second

##sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/UserMall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = second

## 不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k2.hdfs.fileType = CompressedStream 

a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k2.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2
```

## 总结

对该模块进行了简单总结<br>

#### 机器配置情况

由于计算资源有限.我们在16G内存的计算机上搭建了3台虚拟机.每台虚拟机的组件进程在上面的图已经给出.

#### 组件框架的版本

使用Apache提供的软件包

#### Hadoop的总结

- HDFS的读写流程是怎么样的？
- MapReduce的具体过程以及如何优化Hadoop（主要是压缩和小文件的优化）

#### Flume相关

- 优化部分：通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量.

- HDFS Sink的小文件处理：

1）**元数据层面：**每个小文件都有一份元数据,其中包括文件路径,文件名,所有者,所属组,权限,创建时间等,这些信息都保存在Namenode内存中.所以小文件过多,会占用Namenode服务器大量内存，影响Namenode性能和使用寿命.<br>

2）**计算层面：**默认情况下MR会对每个小文件启用一个Map任务计算,非常影响计算性能.同时也影响磁盘寻址时间.<br>

#### Kafka相关

- Kafka消息数据积压，Kafka消费能力不足怎么处理？

（1）如果是Kafka消费能力不足,则可以考虑增加Topic的分区数,并且同时提升消费组的消费者数量,消费者数=分区数.（两者缺一不可）<br>

（2）如果是下游的数据处理不及时：提高每批次拉取的数量.批次拉取数据过少（拉取数据/处理时间<生产速度）,使处理的数据小于生产的数据,也会造成数据积压.































































