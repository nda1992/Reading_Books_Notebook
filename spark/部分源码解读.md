# Spark中的部分源码阅读

## Spark的配置

### 系统属性配置

Spark中的配置主要分为内置的配置和用户自定义配置.<br>

spark中的系统配置类为SparkConf.而spark中的每个组件都直接或间接的使用它存储属性，这些属性都存储在如下的数据结构中:

```scala
//key或value的来源都是String类型
private val setting = new ConcurrentHashMap[String,String]()
```

loadDefaults为Boolean类型的构造器属性，将会从系统中加载spark配置:

```scala
if(loadDefaults){
    loadFromSystemProperties(false)
}
private[spark] def loadFromSystemProperties(silent:Boolean):SparkConf={
    for((key,value)<-Utils.getSystemProperties if key.startWith("spark.")){		//获取spark.字符串为前缀的key和value，并调用set方法
        set(key,value,silent)
    }
    this
}
//设置到settings中
private[spark] def set(key:String,value:String,silent:Boolean):SparkConf{
    if(key==null){
        throw new ...
    }
    if(value==null){
        throw new ...
    }
    if(!silent){
        logPrecationWarning(key)
    }
    settings.put(key,value)
    this
}
```

### 使用API配置

```scala
//spark中的有些API最终实际调用了set重载方法
def set(key:String,value:String):SparkConf={
    set(key,value,false)
}

//SparkConf中的setMaster、setAppName、setJars等方法都是通过上面的set方法完成spark配置的

//setMaster
def setMaster(master:String):SparkConf={
    set("spark.master",master)
}

//setAppName
def setMaster(name:String):SparkConf={
    set("spark.app.name",master)
}
```

### 克隆SparkConf配置

```scala
//为了解决并发中存在的性能问题，所以当SparkConf实例被多个组件所共用时，采用了克隆的方法
//SparkConf继承了Cloneable，并重写clone方法
override def clone:SparkConf={
    val cloned = new SparkConf(false)
    settings.entryset().asScala.foreach{
        e=>cloned.set(e.getKey(),e.getValue(),true)
    }
}
```

### spark的RPC

RPC就是从一台机器（客户端）上通过参数传递的方式调用另一台机器（服务器）上的一个函数或方法（可以统称为服务）并得到返回的结果，RPC实现了互联网上多台计算机的可以相互通信的操作.































