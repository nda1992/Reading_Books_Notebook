# 机器学习常用算法

主要记录了一些疑难点.没有很详细的过程.<br>

代码的实现在该目录下的\*.py文件

## 决策树

### 信息熵

某个特征的熵的值越高，表示该类别的分类能力就越强.<br>

类别$x_i$的信息定义为：
$$
l(x_i)=-log_2p(x_i)
$$
$p(x_i)$表示该类别的概率.<br>

将所有类别相加.就得到该特征下的信息熵（即期望值）：
$$
H=-\sum\limits_{i=1}^np(x_i)log_2(p(x_i))
$$
那么使用$H(D)$表示数据集$D$的信息熵.$|D|$表示样本容量.而$k$个类$C_k$，$k=1,2,3,...,k$,$|C_k|$为属于类$C_k​$的样本个数.因此，得到的经验熵公式为：
$$
H(D)=-\sum\limits_{k=1}^k(\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|})
$$
例如：某个特征有$15$个数据，只有两个类别，一个类别占$9$个，另一个类别占$6$个，则数据集$D$的信息熵为：
$$
H(D)=-\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15}=0.971
$$



### 信息增益

#### 条件熵

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性.随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$,定义给定条件$Y$下的条件概率分布的熵对$X$的数学期望：
$$
H(Y|X)=\sum\limits_n^np_iH(Y|X=x_i)
$$
其中：
$$
p_i=P(X=x_i),i=1,2,...,n
$$
对于特征A对训练集$D$的信心增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下$D$的经验条件熵$H(D,A)$之差：
$$
g(D,A)=H(D)-H(D|A)
$$
公式$(7)$也称为熵与条件熵的互信息.

设特征A有$n$个不同的取值${a_1,a_2,···,a_n}$，根据特征A的取值将$D$划分为$n$个子集$D_1,D_2，···,D_n$，$|D_i|$为$D_i$的样本个数。记子集$D_i$中属于$C_k$的样本的集合为$D_{ik}$，即$D_{ik} = {D_i } \cap{C_k}$，$|D_{ik}|$为$D_{ik}$的样本个数。于是经验条件熵的公式可以表示为:


























